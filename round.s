// Code generated by command: go run main.go. DO NOT EDIT.

#include "textflag.h"

DATA iv<>+0(SB)/4, $0x6a09e667
DATA iv<>+4(SB)/4, $0xbb67ae85
DATA iv<>+8(SB)/4, $0x3c6ef372
DATA iv<>+12(SB)/4, $0xa54ff53a
DATA iv<>+16(SB)/4, $0x510e527f
DATA iv<>+20(SB)/4, $0x9b05688c
DATA iv<>+24(SB)/4, $0x1f83d9ab
DATA iv<>+28(SB)/4, $0x5be0cd19
GLOBL iv<>(SB), RODATA|NOPTR, $32

DATA rot16_shuf<>+0(SB)/1, $0x02
DATA rot16_shuf<>+1(SB)/1, $0x03
DATA rot16_shuf<>+2(SB)/1, $0x00
DATA rot16_shuf<>+3(SB)/1, $0x01
DATA rot16_shuf<>+4(SB)/1, $0x06
DATA rot16_shuf<>+5(SB)/1, $0x07
DATA rot16_shuf<>+6(SB)/1, $0x04
DATA rot16_shuf<>+7(SB)/1, $0x05
DATA rot16_shuf<>+8(SB)/1, $0x0a
DATA rot16_shuf<>+9(SB)/1, $0x0b
DATA rot16_shuf<>+10(SB)/1, $0x08
DATA rot16_shuf<>+11(SB)/1, $0x09
DATA rot16_shuf<>+12(SB)/1, $0x0e
DATA rot16_shuf<>+13(SB)/1, $0x0f
DATA rot16_shuf<>+14(SB)/1, $0x0c
DATA rot16_shuf<>+15(SB)/1, $0x0d
DATA rot16_shuf<>+16(SB)/1, $0x12
DATA rot16_shuf<>+17(SB)/1, $0x13
DATA rot16_shuf<>+18(SB)/1, $0x10
DATA rot16_shuf<>+19(SB)/1, $0x11
DATA rot16_shuf<>+20(SB)/1, $0x16
DATA rot16_shuf<>+21(SB)/1, $0x17
DATA rot16_shuf<>+22(SB)/1, $0x14
DATA rot16_shuf<>+23(SB)/1, $0x15
DATA rot16_shuf<>+24(SB)/1, $0x1a
DATA rot16_shuf<>+25(SB)/1, $0x1b
DATA rot16_shuf<>+26(SB)/1, $0x18
DATA rot16_shuf<>+27(SB)/1, $0x19
DATA rot16_shuf<>+28(SB)/1, $0x1e
DATA rot16_shuf<>+29(SB)/1, $0x1f
DATA rot16_shuf<>+30(SB)/1, $0x1c
DATA rot16_shuf<>+31(SB)/1, $0x1d
GLOBL rot16_shuf<>(SB), RODATA|NOPTR, $32

DATA rot8_shuf<>+0(SB)/1, $0x01
DATA rot8_shuf<>+1(SB)/1, $0x02
DATA rot8_shuf<>+2(SB)/1, $0x03
DATA rot8_shuf<>+3(SB)/1, $0x00
DATA rot8_shuf<>+4(SB)/1, $0x05
DATA rot8_shuf<>+5(SB)/1, $0x06
DATA rot8_shuf<>+6(SB)/1, $0x07
DATA rot8_shuf<>+7(SB)/1, $0x04
DATA rot8_shuf<>+8(SB)/1, $0x09
DATA rot8_shuf<>+9(SB)/1, $0x0a
DATA rot8_shuf<>+10(SB)/1, $0x0b
DATA rot8_shuf<>+11(SB)/1, $0x08
DATA rot8_shuf<>+12(SB)/1, $0x0d
DATA rot8_shuf<>+13(SB)/1, $0x0e
DATA rot8_shuf<>+14(SB)/1, $0x0f
DATA rot8_shuf<>+15(SB)/1, $0x0c
DATA rot8_shuf<>+16(SB)/1, $0x11
DATA rot8_shuf<>+17(SB)/1, $0x12
DATA rot8_shuf<>+18(SB)/1, $0x13
DATA rot8_shuf<>+19(SB)/1, $0x10
DATA rot8_shuf<>+20(SB)/1, $0x15
DATA rot8_shuf<>+21(SB)/1, $0x16
DATA rot8_shuf<>+22(SB)/1, $0x17
DATA rot8_shuf<>+23(SB)/1, $0x14
DATA rot8_shuf<>+24(SB)/1, $0x19
DATA rot8_shuf<>+25(SB)/1, $0x1a
DATA rot8_shuf<>+26(SB)/1, $0x1b
DATA rot8_shuf<>+27(SB)/1, $0x18
DATA rot8_shuf<>+28(SB)/1, $0x1d
DATA rot8_shuf<>+29(SB)/1, $0x1e
DATA rot8_shuf<>+30(SB)/1, $0x1f
DATA rot8_shuf<>+31(SB)/1, $0x1c
GLOBL rot8_shuf<>(SB), RODATA|NOPTR, $32

DATA block_len<>+0(SB)/4, $0x00000040
DATA block_len<>+4(SB)/4, $0x00000040
DATA block_len<>+8(SB)/4, $0x00000040
DATA block_len<>+12(SB)/4, $0x00000040
DATA block_len<>+16(SB)/4, $0x00000040
DATA block_len<>+20(SB)/4, $0x00000040
DATA block_len<>+24(SB)/4, $0x00000040
DATA block_len<>+28(SB)/4, $0x00000040
GLOBL block_len<>(SB), RODATA|NOPTR, $32

// func hash8_avx(inputs *[8]*byte, blocks uint64, key *[8]uint32, counter uint64, inc uint64, flags uint8, flags_start uint8, flags_end uint8, out *[256]byte)
// Requires: AVX, AVX2
TEXT Â·hash8_avx(SB), $616-56
	MOVQ    inputs+0(FP), AX
	MOVQ    blocks+8(FP), CX
	MOVQ    key+16(FP), DX
	MOVQ    counter+24(FP), BX
	MOVQ    inc+32(FP), BP
	MOVBLZX flags+40(FP), SI
	MOVBLZX flags_start+41(FP), DI
	MOVBLZX flags_end+42(FP), R8
	MOVQ    out+48(FP), R9

	// Load key into vectors
	VPBROADCASTD (DX), Y0
	VPBROADCASTD 4(DX), Y1
	VPBROADCASTD 8(DX), Y2
	VPBROADCASTD 12(DX), Y3
	VPBROADCASTD 16(DX), Y4
	VPBROADCASTD 20(DX), Y5
	VPBROADCASTD 24(DX), Y6
	VPBROADCASTD 28(DX), Y7

	// Build and store counter data on the stack
	MOVQ BX, DX
	MOVL DX, 40(SP)
	SHRQ $0x20, DX
	MOVL DX, 72(SP)
	MOVQ BP, DX
	ANDQ $0x01, DX
	ADDQ BX, DX
	MOVL DX, 44(SP)
	SHRQ $0x20, DX
	MOVL DX, 76(SP)
	MOVQ BP, DX
	ANDQ $0x02, DX
	ADDQ BX, DX
	MOVL DX, 48(SP)
	SHRQ $0x20, DX
	MOVL DX, 80(SP)
	MOVQ BP, DX
	ANDQ $0x03, DX
	ADDQ BX, DX
	MOVL DX, 52(SP)
	SHRQ $0x20, DX
	MOVL DX, 84(SP)
	MOVQ BP, DX
	ANDQ $0x04, DX
	ADDQ BX, DX
	MOVL DX, 56(SP)
	SHRQ $0x20, DX
	MOVL DX, 88(SP)
	MOVQ BP, DX
	ANDQ $0x05, DX
	ADDQ BX, DX
	MOVL DX, 60(SP)
	SHRQ $0x20, DX
	MOVL DX, 92(SP)
	MOVQ BP, DX
	ANDQ $0x06, DX
	ADDQ BX, DX
	MOVL DX, 64(SP)
	SHRQ $0x20, DX
	MOVL DX, 96(SP)
	MOVQ BP, DX
	ANDQ $0x07, DX
	ADDQ BX, DX
	MOVL DX, 68(SP)
	SHRQ $0x20, DX
	MOVL DX, 100(SP)

	// Set up block flags for first iteration
	MOVL SI, 32(SP)
	ORL  DI, 32(SP)

loop:
	CMPQ CX, $0x00
	JEQ  finalize

	// Include end flags if last block
	CMPQ CX, $0x01
	JNE  round_setup
	ORL  R8, 32(SP)

round_setup:
	// Load and transpose message vectors
	MOVQ        (AX), DX
	VMOVDQU     (DX), Y8
	MOVQ        8(AX), DX
	VMOVDQU     (DX), Y9
	MOVQ        16(AX), DX
	VMOVDQU     (DX), Y10
	MOVQ        24(AX), DX
	VMOVDQU     (DX), Y11
	MOVQ        32(AX), DX
	VMOVDQU     (DX), Y12
	MOVQ        40(AX), DX
	VMOVDQU     (DX), Y13
	MOVQ        48(AX), DX
	VMOVDQU     (DX), Y14
	MOVQ        56(AX), DX
	VMOVDQU     (DX), Y15
	VMOVDQU     Y0, (SP)
	VPUNPCKLDQ  Y9, Y8, Y0
	VPUNPCKHDQ  Y9, Y8, Y8
	VPUNPCKLDQ  Y11, Y10, Y9
	VPUNPCKHDQ  Y11, Y10, Y10
	VPUNPCKLDQ  Y13, Y12, Y11
	VPUNPCKHDQ  Y13, Y12, Y12
	VPUNPCKLDQ  Y15, Y14, Y13
	VPUNPCKHDQ  Y15, Y14, Y14
	VPUNPCKLQDQ Y9, Y0, Y15
	VPUNPCKHQDQ Y9, Y0, Y0
	VPUNPCKLQDQ Y10, Y8, Y9
	VPUNPCKHQDQ Y10, Y8, Y8
	VPUNPCKLQDQ Y13, Y11, Y10
	VPUNPCKHQDQ Y13, Y11, Y11
	VPUNPCKLQDQ Y14, Y12, Y13
	VPUNPCKHQDQ Y14, Y12, Y12
	VINSERTI128 $0x01, X10, Y15, Y14
	VPERM2I128  $0x31, Y10, Y15, Y10
	VINSERTI128 $0x01, X11, Y0, Y15
	VPERM2I128  $0x31, Y11, Y0, Y0
	VINSERTI128 $0x01, X13, Y9, Y11
	VPERM2I128  $0x31, Y13, Y9, Y9
	VINSERTI128 $0x01, X12, Y8, Y13
	VPERM2I128  $0x31, Y12, Y8, Y8
	VMOVDQU     Y14, 104(SP)
	VMOVDQU     Y15, 136(SP)
	VMOVDQU     Y11, 168(SP)
	VMOVDQU     Y13, 200(SP)
	VMOVDQU     Y10, 232(SP)
	VMOVDQU     Y0, 264(SP)
	VMOVDQU     Y9, 296(SP)
	VMOVDQU     Y8, 328(SP)
	MOVQ        (AX), DX
	VMOVDQU     32(DX), Y14
	MOVQ        8(AX), DX
	VMOVDQU     32(DX), Y15
	MOVQ        16(AX), DX
	VMOVDQU     32(DX), Y11
	MOVQ        24(AX), DX
	VMOVDQU     32(DX), Y13
	MOVQ        32(AX), DX
	VMOVDQU     32(DX), Y10
	MOVQ        40(AX), DX
	VMOVDQU     32(DX), Y0
	MOVQ        48(AX), DX
	VMOVDQU     32(DX), Y9
	MOVQ        56(AX), DX
	VMOVDQU     32(DX), Y8
	VPUNPCKLDQ  Y15, Y14, Y12
	VPUNPCKHDQ  Y15, Y14, Y14
	VPUNPCKLDQ  Y13, Y11, Y15
	VPUNPCKHDQ  Y13, Y11, Y11
	VPUNPCKLDQ  Y0, Y10, Y13
	VPUNPCKHDQ  Y0, Y10, Y0
	VPUNPCKLDQ  Y8, Y9, Y10
	VPUNPCKHDQ  Y8, Y9, Y8
	VPUNPCKLQDQ Y15, Y12, Y9
	VPUNPCKHQDQ Y15, Y12, Y12
	VPUNPCKLQDQ Y11, Y14, Y15
	VPUNPCKHQDQ Y11, Y14, Y11
	VPUNPCKLQDQ Y10, Y13, Y14
	VPUNPCKHQDQ Y10, Y13, Y10
	VPUNPCKLQDQ Y8, Y0, Y13
	VPUNPCKHQDQ Y8, Y0, Y0
	VINSERTI128 $0x01, X14, Y9, Y8
	VPERM2I128  $0x31, Y14, Y9, Y9
	VINSERTI128 $0x01, X10, Y12, Y14
	VPERM2I128  $0x31, Y10, Y12, Y10
	VINSERTI128 $0x01, X13, Y15, Y12
	VPERM2I128  $0x31, Y13, Y15, Y13
	VINSERTI128 $0x01, X0, Y11, Y15
	VPERM2I128  $0x31, Y0, Y11, Y0
	VMOVDQU     Y8, 360(SP)
	VMOVDQU     Y14, 392(SP)
	VMOVDQU     Y12, 424(SP)
	VMOVDQU     Y15, 456(SP)
	VMOVDQU     Y9, 488(SP)
	VMOVDQU     Y10, 520(SP)
	VMOVDQU     Y13, 552(SP)
	VMOVDQU     Y0, 584(SP)
	ADDQ        $0x40, (AX)
	ADDQ        $0x40, 8(AX)
	ADDQ        $0x40, 16(AX)
	ADDQ        $0x40, 24(AX)
	ADDQ        $0x40, 32(AX)
	ADDQ        $0x40, 40(AX)
	ADDQ        $0x40, 48(AX)
	ADDQ        $0x40, 56(AX)

	// Set up block length and flag vectors
	VMOVDQU      block_len<>+0(SB), Y0
	VPBROADCASTD 32(SP), Y8

	// Set up IV vectors
	VPBROADCASTD iv<>+0(SB), Y9
	VPBROADCASTD iv<>+4(SB), Y10
	VPBROADCASTD iv<>+8(SB), Y11
	VPBROADCASTD iv<>+12(SB), Y12

	// Set up counter vectors
	VMOVDQU 40(SP), Y13
	VMOVDQU 72(SP), Y14

	// Round 1
	VMOVDQU (SP), Y15
	VPADDD  104(SP), Y15, Y15
	VPADDD  168(SP), Y1, Y1
	VPADDD  232(SP), Y2, Y2
	VPADDD  296(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  136(SP), Y15, Y15
	VPADDD  200(SP), Y1, Y1
	VPADDD  264(SP), Y2, Y2
	VPADDD  328(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  360(SP), Y15, Y15
	VPADDD  424(SP), Y1, Y1
	VPADDD  488(SP), Y2, Y2
	VPADDD  552(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  392(SP), Y15, Y15
	VPADDD  456(SP), Y1, Y1
	VPADDD  520(SP), Y2, Y2
	VPADDD  584(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 2
	VMOVDQU (SP), Y15
	VPADDD  168(SP), Y15, Y15
	VPADDD  200(SP), Y1, Y1
	VPADDD  328(SP), Y2, Y2
	VPADDD  232(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  296(SP), Y15, Y15
	VPADDD  424(SP), Y1, Y1
	VPADDD  104(SP), Y2, Y2
	VPADDD  520(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  136(SP), Y15, Y15
	VPADDD  488(SP), Y1, Y1
	VPADDD  392(SP), Y2, Y2
	VPADDD  584(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  456(SP), Y15, Y15
	VPADDD  264(SP), Y1, Y1
	VPADDD  552(SP), Y2, Y2
	VPADDD  360(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 3
	VMOVDQU (SP), Y15
	VPADDD  200(SP), Y15, Y15
	VPADDD  424(SP), Y1, Y1
	VPADDD  520(SP), Y2, Y2
	VPADDD  328(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  232(SP), Y15, Y15
	VPADDD  488(SP), Y1, Y1
	VPADDD  168(SP), Y2, Y2
	VPADDD  552(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  296(SP), Y15, Y15
	VPADDD  392(SP), Y1, Y1
	VPADDD  456(SP), Y2, Y2
	VPADDD  360(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  264(SP), Y15, Y15
	VPADDD  104(SP), Y1, Y1
	VPADDD  584(SP), Y2, Y2
	VPADDD  136(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 4
	VMOVDQU (SP), Y15
	VPADDD  424(SP), Y15, Y15
	VPADDD  488(SP), Y1, Y1
	VPADDD  552(SP), Y2, Y2
	VPADDD  520(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  328(SP), Y15, Y15
	VPADDD  392(SP), Y1, Y1
	VPADDD  200(SP), Y2, Y2
	VPADDD  584(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  232(SP), Y15, Y15
	VPADDD  456(SP), Y1, Y1
	VPADDD  264(SP), Y2, Y2
	VPADDD  136(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  104(SP), Y15, Y15
	VPADDD  168(SP), Y1, Y1
	VPADDD  360(SP), Y2, Y2
	VPADDD  296(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 5
	VMOVDQU (SP), Y15
	VPADDD  488(SP), Y15, Y15
	VPADDD  392(SP), Y1, Y1
	VPADDD  584(SP), Y2, Y2
	VPADDD  552(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  520(SP), Y15, Y15
	VPADDD  456(SP), Y1, Y1
	VPADDD  424(SP), Y2, Y2
	VPADDD  360(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  328(SP), Y15, Y15
	VPADDD  264(SP), Y1, Y1
	VPADDD  104(SP), Y2, Y2
	VPADDD  296(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  168(SP), Y15, Y15
	VPADDD  200(SP), Y1, Y1
	VPADDD  136(SP), Y2, Y2
	VPADDD  232(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 6
	VMOVDQU (SP), Y15
	VPADDD  392(SP), Y15, Y15
	VPADDD  456(SP), Y1, Y1
	VPADDD  360(SP), Y2, Y2
	VPADDD  584(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  552(SP), Y15, Y15
	VPADDD  264(SP), Y1, Y1
	VPADDD  488(SP), Y2, Y2
	VPADDD  136(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  520(SP), Y15, Y15
	VPADDD  104(SP), Y1, Y1
	VPADDD  168(SP), Y2, Y2
	VPADDD  232(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  200(SP), Y15, Y15
	VPADDD  424(SP), Y1, Y1
	VPADDD  296(SP), Y2, Y2
	VPADDD  328(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 7
	VMOVDQU (SP), Y15
	VPADDD  456(SP), Y15, Y15
	VPADDD  264(SP), Y1, Y1
	VPADDD  136(SP), Y2, Y2
	VPADDD  360(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  584(SP), Y15, Y15
	VPADDD  104(SP), Y1, Y1
	VPADDD  392(SP), Y2, Y2
	VPADDD  296(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  552(SP), Y15, Y15
	VPADDD  168(SP), Y1, Y1
	VPADDD  200(SP), Y2, Y2
	VPADDD  328(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  424(SP), Y15, Y15
	VPADDD  488(SP), Y1, Y1
	VPADDD  232(SP), Y2, Y2
	VPADDD  520(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Finalize rounds
	VPXOR (SP), Y9, Y9
	VPXOR Y1, Y10, Y1
	VPXOR Y2, Y11, Y2
	VPXOR Y3, Y12, Y3
	VPXOR Y4, Y13, Y4
	VPXOR Y5, Y14, Y5
	VPXOR Y6, Y0, Y0
	VPXOR Y7, Y8, Y6

	// Fix up registers for next iteration
	VMOVDQU Y6, Y7
	VMOVDQU Y0, Y6
	VMOVDQU Y9, Y0

	// Decrement and loop
	DECQ CX
	MOVL SI, 32(SP)
	JMP  loop

finalize:
	// Transpose output vectors
	VPUNPCKLDQ  Y1, Y0, Y8
	VPUNPCKHDQ  Y1, Y0, Y0
	VPUNPCKLDQ  Y3, Y2, Y1
	VPUNPCKHDQ  Y3, Y2, Y2
	VPUNPCKLDQ  Y5, Y4, Y3
	VPUNPCKHDQ  Y5, Y4, Y4
	VPUNPCKLDQ  Y7, Y6, Y5
	VPUNPCKHDQ  Y7, Y6, Y6
	VPUNPCKLQDQ Y1, Y8, Y7
	VPUNPCKHQDQ Y1, Y8, Y1
	VPUNPCKLQDQ Y2, Y0, Y8
	VPUNPCKHQDQ Y2, Y0, Y0
	VPUNPCKLQDQ Y5, Y3, Y2
	VPUNPCKHQDQ Y5, Y3, Y3
	VPUNPCKLQDQ Y6, Y4, Y5
	VPUNPCKHQDQ Y6, Y4, Y4
	VINSERTI128 $0x01, X2, Y7, Y6
	VPERM2I128  $0x31, Y2, Y7, Y2
	VINSERTI128 $0x01, X3, Y1, Y7
	VPERM2I128  $0x31, Y3, Y1, Y1
	VINSERTI128 $0x01, X5, Y8, Y3
	VPERM2I128  $0x31, Y5, Y8, Y5
	VINSERTI128 $0x01, X4, Y0, Y8
	VPERM2I128  $0x31, Y4, Y0, Y0

	// Store into output
	VMOVDQU Y6, (R9)
	VMOVDQU Y7, 32(R9)
	VMOVDQU Y3, 64(R9)
	VMOVDQU Y8, 96(R9)
	VMOVDQU Y2, 128(R9)
	VMOVDQU Y1, 160(R9)
	VMOVDQU Y5, 192(R9)
	VMOVDQU Y0, 224(R9)
	RET
